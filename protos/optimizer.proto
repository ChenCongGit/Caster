syntax = "proto2";
package Caster.protos;


message Optimizer {
  oneof optimizer {
    GradientDescentOptimizer gradient_descent_optimizer = 1;
    RMSPropOptimizer rms_prop_optimizer = 2;
    MomentumOptimizer momentum_optimizer = 3;
    AdamOptimizer adam_optimizer = 4;
    NadamOptimizer nadam_optimizer = 5;
    AdadeltaOptimizer adadelta_optimizer = 6;
  }
  optional bool use_moving_average = 7 [default=true];
  optional float moving_average_decay = 8 [default=0.9999];
}

message GradientDescentOptimizer {
  optional LearningRate learning_rate = 1;
}

message RMSPropOptimizer {
  optional LearningRate learning_rate = 1;
  optional float momentum_optimizer_value = 2 [default=0.9];
  optional float decay = 3 [default=0.9];
  optional float epsilon = 4 [default=1.0];
}

message MomentumOptimizer {
  optional LearningRate learning_rate = 1;
  optional float momentum_optimizer_value = 2 [default=0.9];
}

message AdamOptimizer {
  optional LearningRate learning_rate = 1;
}

message NadamOptimizer {
  optional LearningRate learning_rate = 1;
}

message AdadeltaOptimizer {
  optional LearningRate learning_rate = 1;
  optional float rho = 2 [default=0.95];
}

message LearningRate {
  oneof learning_rate {
    ConstantLearningRate constant_learning_rate = 1;
    ExponentialDecayLearningRate exponential_decay_learning_rate = 2;
    ManualStepLearningRate manual_step_learning_rate = 3;
  }
}

message ConstantLearningRate {
  optional float learning_rate = 1 [default=0.002];
}

message ExponentialDecayLearningRate {
  optional float initial_learning_rate = 1 [default=0.002];
  optional uint32 decay_steps = 2 [default=4000000];
  optional float decay_factor = 3 [default=0.95];
  optional bool staircase = 4 [default=true];
}

message ManualStepLearningRate {
  optional float initial_learning_rate = 1 [default=0.002];
  message LearningRateSchedule {
    optional uint32 step = 1;
    optional float learning_rate = 2 [default=0.002];
  }
  repeated LearningRateSchedule schedule = 2;
}
